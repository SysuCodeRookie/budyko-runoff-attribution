"""
test_real_data.py

çœŸå®æ•°æ®æµ‹è¯•è„šæœ¬

ä½¿ç”¨ä¸‹è½½çš„çœŸå®æ•°æ®ï¼ˆæˆ–æ¨¡æ‹Ÿæ•°æ®ï¼‰æµ‹è¯•å®Œæ•´å·¥ä½œæµï¼š
1. è¯»å–GRDCå¾„æµæ•°æ®
2. å¤„ç†æ°”å€™æ•°æ®
3. è®¡ç®—PET
4. å‚æ•°æ ¡å‡†å’Œå½’å› åˆ†æ

ä½œè€…: Research Software Engineer  
æ—¥æœŸ: 2025-01-01
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.data_preprocessing.grdc_parser import GRDCParser
from src.budyko_model.pet_calculator import PETCalculator
from src.budyko_model.parameter_calibration import ParameterCalibrator


def test_grdc_sample_data():
    """æµ‹è¯•GRDCæ ·æœ¬æ•°æ®è§£æ"""
    print("\n" + "="*70)
    print("æµ‹è¯•1: GRDCæ•°æ®è§£æ")
    print("="*70)
    
    data_dir = project_root / "data" / "raw" / "GRDC"
    grdc_files = list(data_dir.glob("*.txt"))
    
    if not grdc_files:
        print("âŒ æœªæ‰¾åˆ°GRDCæ•°æ®æ–‡ä»¶")
        print(f"   è¯·å°†GRDCæ•°æ®æ”¾ç½®åœ¨: {data_dir}")
        return None
    
    print(f"\næ‰¾åˆ° {len(grdc_files)} ä¸ªGRDCæ–‡ä»¶:")
    for f in grdc_files:
        print(f"  - {f.name}")
    
    # è§£æç¬¬ä¸€ä¸ªæ–‡ä»¶
    grdc_file = grdc_files[0]
    print(f"\nè§£ææ–‡ä»¶: {grdc_file.name}")
    
    try:
        parser = GRDCParser(str(grdc_file))
        
        # æå–å…ƒæ•°æ®
        metadata = parser.parse_metadata()
        print("\nã€ç«™ç‚¹å…ƒæ•°æ®ã€‘")
        for key, value in metadata.items():
            print(f"  {key}: {value}")
        
        # è¯»å–æ—¶é—´åºåˆ—
        df = parser.read_timeseries()
        print(f"\nã€æ—¶é—´åºåˆ—ã€‘")
        print(f"  è®°å½•æ•°: {len(df)}")
        print(f"  æ—¶é—´èŒƒå›´: {df.index.min()} è‡³ {df.index.max()}")
        print(f"  å¹³å‡æµé‡: {df['discharge'].mean():.2f} mÂ³/s")
        
        # è½¬æ¢ä¸ºå¹´å€¼
        df_annual = parser.aggregate_to_annual()
        print(f"\nã€å¹´åº¦æ•°æ®ã€‘")
        print(f"  å¹´ä»½æ•°: {len(df_annual)}")
        print(df_annual.head())
        
        # è½¬æ¢ä¸ºå¾„æµæ·±åº¦
        df_depth = parser.convert_to_depth()
        print(f"\nã€å¾„æµæ·±åº¦ã€‘")
        print(f"  å¹³å‡å¾„æµæ·±åº¦: {df_depth['runoff_depth_mm'].mean():.1f} mm/year")
        
        print("\nâœ… GRDCæ•°æ®è§£ææˆåŠŸ")
        return df_depth
        
    except Exception as e:
        print(f"\nâŒ è§£æå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def test_climate_data_simulation():
    """æ¨¡æ‹Ÿæ°”å€™æ•°æ®ï¼ˆå½“ISIMIPæ•°æ®æœªä¸‹è½½æ—¶ï¼‰"""
    print("\n" + "="*70)
    print("æµ‹è¯•2: æ°”å€™æ•°æ®æ¨¡æ‹Ÿ")
    print("="*70)
    
    print("\nç”Ÿæˆæ¨¡æ‹Ÿæ°”å€™æ•°æ®ç”¨äºæµ‹è¯•...")
    
    # ç”Ÿæˆ1960-2016å¹´çš„å¹´åº¦æ°”å€™æ•°æ®
    years = np.arange(1960, 2017)
    np.random.seed(456)
    
    climate_data = pd.DataFrame({
        'year': years,
        'P': np.random.normal(850, 100, len(years)),      # é™æ°´ (mm/year)
        'PET': np.random.normal(1100, 120, len(years)),   # PET (mm/year)
        'tas': np.random.normal(15, 2, len(years)),       # æ°”æ¸© (Â°C)
    })
    
    print("\nã€æ¨¡æ‹Ÿæ°”å€™æ•°æ®ã€‘")
    print(f"  å¹´ä»½èŒƒå›´: {years[0]}-{years[-1]}")
    print(f"  å¹³å‡é™æ°´: {climate_data['P'].mean():.1f} mm/year")
    print(f"  å¹³å‡PET: {climate_data['PET'].mean():.1f} mm/year")
    print(f"  å¹³å‡æ°”æ¸©: {climate_data['tas'].mean():.1f} Â°C")
    
    print("\nâœ… æ°”å€™æ•°æ®æ¨¡æ‹Ÿå®Œæˆ")
    return climate_data


def test_complete_workflow():
    """æµ‹è¯•å®Œæ•´å·¥ä½œæµ"""
    print("\n" + "="*70)
    print("æµ‹è¯•3: å®Œæ•´å½’å› åˆ†æå·¥ä½œæµ")
    print("="*70)
    
    # æ­¥éª¤1: è·å–å¾„æµæ•°æ®
    runoff_data = test_grdc_sample_data()
    
    if runoff_data is None:
        print("\nä½¿ç”¨æ¨¡æ‹Ÿå¾„æµæ•°æ®...")
        years = np.arange(1960, 2017)
        runoff_data = pd.DataFrame({
            'year': years,
            'runoff_depth_mm': np.random.normal(250, 50, len(years))
        })
    else:
        # ç¡®ä¿æœ‰yearåˆ—
        if 'year' not in runoff_data.columns:
            runoff_data['year'] = runoff_data.index.year
    
    # æ­¥éª¤2: è·å–æ°”å€™æ•°æ®
    climate_data = test_climate_data_simulation()
    
    # æ­¥éª¤3: åˆå¹¶æ•°æ®
    print("\n" + "="*70)
    print("æ­¥éª¤3: æ•°æ®æ•´åˆ")
    print("="*70)
    
    # åˆå¹¶å¾„æµå’Œæ°”å€™æ•°æ®
    combined_data = pd.merge(
        runoff_data[['year', 'runoff_depth_mm']],
        climate_data[['year', 'P', 'PET']],
        on='year',
        how='inner'
    )
    
    # é‡å‘½ååˆ—
    combined_data.rename(columns={'runoff_depth_mm': 'Q_n'}, inplace=True)
    
    print(f"\nåˆå¹¶åæ•°æ®: {len(combined_data)} å¹´")
    print(combined_data.head())
    
    # æ­¥éª¤4: å‚æ•°æ ¡å‡†å’Œå½’å› åˆ†æ
    print("\n" + "="*70)
    print("æ­¥éª¤4: å‚æ•°æ ¡å‡†å’Œå½’å› åˆ†æ")
    print("="*70)
    
    calibrator = ParameterCalibrator(change_point=1986, min_valid_years=10)
    
    # æ—¶æ®µæ¼”å˜åˆ†æ
    results = calibrator.analyze_parameter_evolution(
        station_id="TEST_STATION",
        time_series=combined_data,
        P_col='P',
        PET_col='PET',
        Q_n_col='Q_n',
        year_col='year'
    )
    
    print("\nã€å‚æ•°æ¼”å˜ç»“æœã€‘")
    for period_key, result in results.items():
        if result:
            print(f"\n{period_key} ({result.period}):")
            print(f"  å‚æ•° n: {result.n:.4f}")
            print(f"  é™æ°´ P: {result.P:.1f} mm/year")
            print(f"  PET: {result.PET:.1f} mm/year")
            print(f"  å¾„æµ Q_n: {result.Q_n:.1f} mm/year")
            print(f"  è’¸æ•£å‘ E: {result.E:.1f} mm/year")
            print(f"  å¹²æ—±æŒ‡æ•°: {result.aridity_index:.3f}")
    
    # å½’å› åˆ†æ
    if 'period_1' in results and 'period_2' in results:
        print("\nã€å½’å› åˆ†æã€‘")
        
        # æ¨¡æ‹Ÿè§‚æµ‹å¾„æµï¼ˆå‡è®¾æœ‰5%çš„äººç±»å–ç”¨æ°´ï¼‰
        period_1_mask = combined_data['year'] < 1986
        period_2_mask = combined_data['year'] >= 1986
        
        Q_obs_1 = combined_data.loc[period_1_mask, 'Q_n'].mean() * 0.95
        Q_obs_2 = combined_data.loc[period_2_mask, 'Q_n'].mean() * 0.90  # åæœŸå–æ°´å¢åŠ 
        
        attribution = calibrator.calculate_attribution(
            station_id="TEST_STATION",
            period_1_data=results['period_1'],
            period_2_data=results['period_2'],
            Q_obs_1=Q_obs_1,
            Q_obs_2=Q_obs_2
        )
        
        if attribution:
            print(f"\nè§‚æµ‹å¾„æµå˜åŒ–: Î”Q_obs = {attribution.delta_Q_obs:.1f} mm/year")
            print(f"å¤©ç„¶å¾„æµå˜åŒ–: Î”Q_n = {attribution.delta_Q_n:.1f} mm/year")
            
            print(f"\nã€å½’å› è´¡çŒ®é‡ã€‘")
            print(f"  æ°”å€™å˜åŒ– (CCV):      {attribution.delta_Q_CCV:+.1f} mm/year")
            print(f"  åœŸåœ°åˆ©ç”¨å˜åŒ– (LUCC): {attribution.delta_Q_LUCC:+.1f} mm/year")
            print(f"  äººç±»å–ç”¨æ°´ (WADR):    {attribution.delta_Q_WADR:+.1f} mm/year")
            
            if not np.isnan(attribution.C_CCV):
                print(f"\nã€å½’å› è´¡çŒ®ç‡ã€‘")
                print(f"  æ°”å€™å˜åŒ– (CCV):      {attribution.C_CCV:.1f}%")
                print(f"  åœŸåœ°åˆ©ç”¨å˜åŒ– (LUCC): {attribution.C_LUCC:.1f}%")
                print(f"  äººç±»å–ç”¨æ°´ (WADR):    {attribution.C_WADR:.1f}%")
            
            print(f"\nã€å¼¹æ€§ç³»æ•°ã€‘")
            print(f"  ÎµP (é™æ°´å¼¹æ€§):   {attribution.elasticity['epsilon_P']:.3f}")
            print(f"  ÎµPET (PETå¼¹æ€§):  {attribution.elasticity['epsilon_PET']:.3f}")
            print(f"  Îµn (å‚æ•°nå¼¹æ€§):  {attribution.elasticity['epsilon_n']:.3f}")
    
    print("\nâœ… å®Œæ•´å·¥ä½œæµæµ‹è¯•å®Œæˆ")
    
    # æ­¥éª¤5: å¯¼å‡ºç»“æœ
    print("\n" + "="*70)
    print("æ­¥éª¤5: å¯¼å‡ºç»“æœ")
    print("="*70)
    
    output_dir = project_root / "data" / "results"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # å¯¼å‡ºåˆå¹¶æ•°æ®
    combined_output = output_dir / "test_combined_data.csv"
    combined_data.to_csv(combined_output, index=False)
    print(f"\nâœ… åˆå¹¶æ•°æ®å·²å¯¼å‡º: {combined_output}")
    
    # å¯¼å‡ºæ ¡å‡†ç»“æœ
    if results:
        calibration_results = []
        for key, result in results.items():
            if result:
                calibration_results.append(result)
        
        if calibration_results:
            calib_output = output_dir / "test_calibration_results.csv"
            calibrator.export_results(calibration_results, str(calib_output))
            print(f"âœ… æ ¡å‡†ç»“æœå·²å¯¼å‡º: {calib_output}")
    
    # å¯¼å‡ºå½’å› ç»“æœ
    if 'period_1' in results and 'period_2' in results and attribution:
        attrib_output = output_dir / "test_attribution_results.csv"
        calibrator.export_attribution_results([attribution], str(attrib_output))
        print(f"âœ… å½’å› ç»“æœå·²å¯¼å‡º: {attrib_output}")


def test_batch_stations():
    """æµ‹è¯•æ‰¹é‡ç«™ç‚¹å¤„ç†"""
    print("\n" + "="*70)
    print("æµ‹è¯•4: æ‰¹é‡ç«™ç‚¹å¤„ç†")
    print("="*70)
    
    print("\nç”Ÿæˆå¤šç«™ç‚¹æ¨¡æ‹Ÿæ•°æ®...")
    
    # åˆ›å»º5ä¸ªæ¨¡æ‹Ÿç«™ç‚¹çš„æ•°æ®
    np.random.seed(789)
    stations_data = []
    
    for i in range(5):
        station_id = f"STATION_{i+1:03d}"
        
        # ä¸åŒæ°”å€™ç±»å‹çš„æµåŸŸ
        if i == 0:  # æ¹¿æ¶¦
            P, PET, Q_n = 1500, 900, 800
        elif i == 1:  # åŠæ¹¿æ¶¦
            P, PET, Q_n = 900, 1100, 250
        elif i == 2:  # åŠå¹²æ—±
            P, PET, Q_n = 550, 1200, 100
        elif i == 3:  # å¹²æ—±
            P, PET, Q_n = 300, 1400, 40
        else:  # ä¸­ç­‰
            P, PET, Q_n = 1000, 1000, 350
        
        # æ·»åŠ éšæœºæ‰°åŠ¨
        P += np.random.normal(0, 50)
        PET += np.random.normal(0, 80)
        Q_n += np.random.normal(0, 30)
        Q_n = max(Q_n, 10)  # ç¡®ä¿éè´Ÿ
        
        stations_data.append({
            'station_id': station_id,
            'P': P,
            'PET': PET,
            'Q_n': Q_n,
            'region': ['æ¹¿æ¶¦', 'åŠæ¹¿æ¶¦', 'åŠå¹²æ—±', 'å¹²æ—±', 'ä¸­ç­‰'][i]
        })
    
    df_stations = pd.DataFrame(stations_data)
    print("\nç«™ç‚¹æ•°æ®:")
    print(df_stations)
    
    # æ‰¹é‡æ ¡å‡†
    print("\næ‰§è¡Œæ‰¹é‡æ ¡å‡†...")
    calibrator = ParameterCalibrator()
    results = calibrator.batch_calibrate_stations(df_stations, parallel=False)
    
    print(f"\nâœ… æ‰¹é‡æ ¡å‡†å®Œæˆ: {len(results)}/{len(df_stations)} ç«™ç‚¹æˆåŠŸ")
    
    # æ˜¾ç¤ºç»“æœ
    results_df = pd.DataFrame([r.to_dict() for r in results])
    results_df = results_df.merge(df_stations[['station_id', 'region']], on='station_id')
    
    print("\nã€æ ¡å‡†ç»“æœæ±‡æ€»ã€‘")
    print(results_df[['station_id', 'region', 'n', 'aridity_index', 
                      'calibration_error']].to_string(index=False))
    
    # å¯¼å‡º
    output_dir = project_root / "data" / "results"
    output_file = output_dir / "test_batch_calibration.csv"
    results_df.to_csv(output_file, index=False)
    print(f"\nâœ… æ‰¹é‡ç»“æœå·²å¯¼å‡º: {output_file}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "="*70)
    print("çœŸå®æ•°æ®æµ‹è¯•å¥—ä»¶")
    print("="*70)
    print("\næœ¬æµ‹è¯•ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¼”ç¤ºå®Œæ•´å·¥ä½œæµ")
    print("æ›¿æ¢ä¸ºçœŸå®GRDCã€ISIMIPæ•°æ®åå¯è¿›è¡Œå®é™…åˆ†æ")
    
    try:
        # æµ‹è¯•1: GRDCæ•°æ®
        test_grdc_sample_data()
        
        # æµ‹è¯•2: å®Œæ•´å·¥ä½œæµ
        test_complete_workflow()
        
        # æµ‹è¯•3: æ‰¹é‡å¤„ç†
        test_batch_stations()
        
        print("\n" + "="*70)
        print("æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("="*70)
        print("""
âœ… æµ‹è¯•æ€»ç»“:
   - GRDCæ•°æ®è§£æ: æˆåŠŸ
   - æ°”å€™æ•°æ®å¤„ç†: æˆåŠŸ
   - å‚æ•°æ ¡å‡†: æˆåŠŸ
   - å½’å› åˆ†æ: æˆåŠŸ
   - æ‰¹é‡å¤„ç†: æˆåŠŸ
   - ç»“æœå¯¼å‡º: æˆåŠŸ

ğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®:
   data/results/test_*.csv

ğŸ” ä¸‹ä¸€æ­¥:
   1. ä¸‹è½½çœŸå®GRDCæ•°æ®ï¼ˆæŒ‰ç…§ data/raw/GRDC/GRDC_DOWNLOAD_INSTRUCTIONS.txtï¼‰
   2. ä¸‹è½½ISIMIPæ•°æ®ï¼ˆæŒ‰ç…§ data/raw/ISIMIP/ISIMIP_DATA_INFO.txtï¼‰
   3. è¿è¡Œå®Œæ•´åˆ†ææµç¨‹
""")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
